{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wikipediaapi\n",
    "import os\n",
    "import shutil\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import json\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectBiographiesFromCategories(all_categories, categorymembers, category, data, number_people, level=0, max_level=1): \n",
    "    \"\"\"\n",
    "    Recursively collects biographical texts and fact graphs for each person in specified Wikipedia category members up to a given recursion level.\n",
    "\n",
    "    Parameters:\n",
    "        categorymembers (dict): A dictionary of category member pages from the WikipediaAPI.\n",
    "        category (str): The name of the category being processed.\n",
    "        data (list): A list to store biographical text and associated metadata.\n",
    "        number_people (int): Counter for the number of biographies collected.\n",
    "        level (int): Current level of recursion through category members.\n",
    "        max_level (int): Maximum depth of recursion allowed.\n",
    "\n",
    "    Returns:\n",
    "        int: Updated count of processed biographies.\n",
    "    \"\"\"\n",
    "    for page in categorymembers.values():\n",
    "        if page.ns == 0 and 'List' not in page.title and number_people < 130: #limit the number of people for each category to 130 (or more for clustering)\n",
    "\n",
    "            #some wikipedia page cannot be found in dbpedia, we ignore them\n",
    "            try: \n",
    "\n",
    "                #knowledge graph of facts\n",
    "                kg_graph = fetchDbpediaFacts(page.pageid, page.title)\n",
    "                # Save the graph of facts to a JSON file\n",
    "                #with open('knowledge_graph.json', 'a') as json_file:\n",
    "                #    json.dump(kg_graph, json_file, indent=4)\n",
    "                \n",
    "                all_categories[category].append(kg_graph)\n",
    "                print(f\"{category} {page.title} processed\")\n",
    "\n",
    "\n",
    "                text = page.text\n",
    "                data.append({'text': text, 'category': category, 'person': (page.title).replace(' ', '')})\n",
    "                #creation of txt file for the biography\n",
    "                f = open(f\"Biographies_{category}/{(page.title).replace(' ', '')}_{category}.txt\", \"w\", encoding='utf-8')\n",
    "                f.write(text)\n",
    "                f.close()\n",
    "\n",
    "                #keep track of people processed\n",
    "                number_people += 1  \n",
    "\n",
    "            except Exception as err:\n",
    "                print(f\"Unexpected {err=}, {type(err)=}, could not process {page.title}\")\n",
    "                continue\n",
    "\n",
    "            \n",
    "        if page.ns == wikipediaapi.Namespace.CATEGORY and level < max_level and number_people < 130: #change to 130\n",
    "            # Recursively call the function to process pages in the subcategory \n",
    "            number_people, all_categories = collectBiographiesFromCategories(all_categories, page.categorymembers, category, data, number_people, level=level + 1, max_level=max_level)\n",
    "    # return updated count of processed people\n",
    "    return number_people, all_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBiographyDataFrames():\n",
    "    \"\"\"\n",
    "    Collects biographical data from Wikipedia categories 'Sculptors' and 'Journalists', \n",
    "    creates directories for storing these biographies, and formats the collected data into a pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A pandas DataFrame containing the text and category of each biography collected.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    wiki = wikipediaapi.Wikipedia('Mozilla/5.0', 'en')\n",
    "\n",
    "    data = []\n",
    "    all_categories= {}\n",
    "    all_categories[\"Journalists\"] = []\n",
    "    all_categories[\"Sculptors\"] = []\n",
    "    \n",
    "    if os.path.exists('Biographies'):\n",
    "        shutil.rmtree('Biographies')\n",
    "    \n",
    "    os.mkdir('Biographies')\n",
    "    os.chdir('Biographies')\n",
    "        \n",
    "    sculptors = wiki.page(\"Category:Sculptors\")\n",
    "    if not os.path.exists('Biographies_Sculptors'):\n",
    "        os.mkdir('Biographies_Sculptors') #creates a directory to store all sculptors' biographies\n",
    "    _, all_categories = collectBiographiesFromCategories(all_categories, sculptors.categorymembers, \"Sculptors\", data, 0)\n",
    "    \n",
    "    journalists = wiki.page(\"Category:Journalists\")\n",
    "    if not os.path.exists('Biographies_Journalists'):\n",
    "        os.mkdir('Biographies_Journalists') #creates a directory to store all journalists' biographies\n",
    "    _, all_categories = collectBiographiesFromCategories(all_categories, journalists.categorymembers, \"Journalists\", data, 0)\n",
    "\n",
    "    print(all_categories)\n",
    "    \n",
    "    with open('knowledge_graph.json', 'w') as json_file:\n",
    "            json.dump(all_categories, json_file, indent=4)\n",
    "    \n",
    "    os.chdir('..')\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchDbpediaFacts(wiki_id, page_title):\n",
    "    \"\"\"\n",
    "    Fetches RDF triples (facts) about a specific Wikipedia page from DBpedia using the page's ID.\n",
    "\n",
    "    Parameters:\n",
    "        wiki_id (int): The Wikipedia page ID used to fetch the corresponding DBpedia page.\n",
    "        page_title (str): Title of the Wikipedia page.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing RDF triples with DBpedia facts about the page, its title and other metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    #initialize the SPARQL wrapper\n",
    "    sparql = SPARQLWrapper(\"http://dbpedia.org/sparql/\")\n",
    "\n",
    "    #first query to retrieve the dbpedia page of the person from the wikipedia page id\n",
    "    sparql.setQuery(f\"\"\"\n",
    "            PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "            PREFIX dbr: <http://dbpedia.org/resource/>\n",
    "            PREFIX dbp: <http://dbpedia.org/property/>\n",
    "\n",
    "            SELECT *\n",
    "            WHERE {{\n",
    "                    ?person dbo:wikiPageID {wiki_id} .\n",
    "            }}\n",
    "    \"\"\")\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    result_link = sparql.query().convert()\n",
    "\n",
    "    person_wikidata_page = result_link[\"results\"][\"bindings\"][0][\"person\"][\"value\"] #the first value is most likely what we are looking for\n",
    "    \n",
    "    #second query to fetch all information as RDF triples\n",
    "    sparql.setQuery(f\"\"\"\n",
    "            PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "            PREFIX dbr: <http://dbpedia.org/resource/>\n",
    "            PREFIX dbp: <http://dbpedia.org/property/>\n",
    "\n",
    "            DESCRIBE <{person_wikidata_page}>\n",
    "    \"\"\")\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results_facts = sparql.query().convert()\n",
    "    \n",
    "    results_facts['results']['bindings'] = [entry for entry in results_facts['results']['bindings']\n",
    "        if ('lang' not in entry['o'].keys() or entry['o']['lang'] == 'en')\n",
    "        and ('lang' not in entry['p'].keys() or entry['p']['lang'] == 'en')\n",
    "        and ('lang' not in entry['s'].keys() or entry['s']['lang'] == 'en')][:100]  #first 100 english facts only\n",
    "\n",
    "    results_facts['head']['person'] = page_title.replace(\" \", \"\")\n",
    "    \n",
    "    return(results_facts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
