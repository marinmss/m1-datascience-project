{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d67785c-6003-402c-8c57-9e3a5420d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "path = './Biographies/*'\n",
    "files = sum([glob.glob(dir+'/*.txt') for dir in glob.glob(path)],[])\n",
    "\n",
    "filenames = [file.split('/')[3] for file in files]\n",
    "\n",
    "ents_spacy=[]\n",
    "for file in files:\n",
    "    with open(file) as infile:\n",
    "        file_content =  infile.read()\n",
    "        doc = nlp(file_content)\n",
    "        ents_spacy.append(doc.ents)\n",
    "\n",
    "data_spacy = list(zip(filenames,ents))\n",
    "df_spacy = pd.DataFrame(data_spacy, columns = ['filename','ents'])\n",
    "print(df_spacy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e63d636d-17ef-4e85-ba1e-c7cfedd456ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marina/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 10.3MB/s]  \n",
      "2024-06-11 22:56:00 INFO: Downloaded file to /home/marina/stanza_resources/resources.json\n",
      "2024-06-11 22:56:00 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-06-11 22:56:02 INFO: File exists: /home/marina/stanza_resources/en/default.zip\n",
      "2024-06-11 22:56:05 INFO: Finished downloading models and saved to /home/marina/stanza_resources\n",
      "2024-06-11 22:56:05 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 3.52MB/s]  \n",
      "2024-06-11 22:56:06 INFO: Downloaded file to /home/marina/stanza_resources/resources.json\n",
      "2024-06-11 22:56:07 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-06-11 22:56:07 INFO: Using device: cpu\n",
      "2024-06-11 22:56:07 INFO: Loading: tokenize\n",
      "2024-06-11 22:56:08 INFO: Loading: mwt\n",
      "2024-06-11 22:56:08 INFO: Loading: pos\n",
      "2024-06-11 22:56:08 INFO: Loading: lemma\n",
      "2024-06-11 22:56:08 INFO: Loading: constituency\n",
      "2024-06-11 22:56:08 INFO: Loading: depparse\n",
      "2024-06-11 22:56:08 INFO: Loading: sentiment\n",
      "2024-06-11 22:56:09 INFO: Loading: ner\n",
      "2024-06-11 22:56:09 INFO: Done loading processors!\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline(lang=\"en\") \n",
    "\n",
    "ents_stanza=[]\n",
    "for file in files:\n",
    "    with open(file) as infile:\n",
    "        file_content =  infile.read()\n",
    "        doc = nlp(file_content)\n",
    "        ents_stanza.append(doc.ents)\n",
    "\n",
    "data_stanza = list(zip(filenames,ents))\n",
    "df_stanza = pd.DataFrame(data, columns = ['filename','ents'])\n",
    "print(df_stanza.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdecdd6e-8dd0-4583-a001-8c06ba7f4413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 23:57:08 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 601kB/s]   \n",
      "2024-06-11 23:57:09 INFO: Downloaded file to /home/marina/stanza_resources/resources.json\n",
      "2024-06-11 23:57:10 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-06-11 23:57:10 INFO: Using device: cpu\n",
      "2024-06-11 23:57:10 INFO: Loading: tokenize\n",
      "2024-06-11 23:57:10 INFO: Loading: mwt\n",
      "2024-06-11 23:57:10 INFO: Loading: pos\n",
      "2024-06-11 23:57:10 INFO: Loading: lemma\n",
      "2024-06-11 23:57:10 INFO: Loading: constituency\n",
      "2024-06-11 23:57:11 INFO: Loading: depparse\n",
      "2024-06-11 23:57:11 INFO: Loading: sentiment\n",
      "2024-06-11 23:57:11 INFO: Loading: ner\n",
      "2024-06-11 23:57:12 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Biographies/Biographies_Journalists/AquileoJ.Echeverría_Journalists.txt\n",
      "./Biographies/Biographies_Journalists/ThomasHenryShadwellClerke_Journalists.txt\n",
      "./Biographies/Biographies_Journalists/Journalist_Journalists.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>ents_stanza</th>\n",
       "      <th>ents_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AquileoJ.Echeverría_Journalists.txt</td>\n",
       "      <td>[(Aquileo J. Echeverría, PERSON), (May 22, 186...</td>\n",
       "      <td>[(Aquileo J. Echeverría, PERSON), (May 22, 186...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ThomasHenryShadwellClerke_Journalists.txt</td>\n",
       "      <td>[(Thomas Henry Shadwell Clerke, PERSON), (KH, ...</td>\n",
       "      <td>[(Thomas Henry Shadwell Clerke, PERSON), (KH, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Journalist_Journalists.txt</td>\n",
       "      <td>[(Matthew C. Nisbet, PERSON), (Walter Lippmann...</td>\n",
       "      <td>[(Matthew C. Nisbet, PERSON), (Walter Lippmann...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   file_name  \\\n",
       "0        AquileoJ.Echeverría_Journalists.txt   \n",
       "1  ThomasHenryShadwellClerke_Journalists.txt   \n",
       "2                 Journalist_Journalists.txt   \n",
       "\n",
       "                                         ents_stanza  \\\n",
       "0  [(Aquileo J. Echeverría, PERSON), (May 22, 186...   \n",
       "1  [(Thomas Henry Shadwell Clerke, PERSON), (KH, ...   \n",
       "2  [(Matthew C. Nisbet, PERSON), (Walter Lippmann...   \n",
       "\n",
       "                                          ents_spacy  \n",
       "0  [(Aquileo J. Echeverría, PERSON), (May 22, 186...  \n",
       "1  [(Thomas Henry Shadwell Clerke, PERSON), (KH, ...  \n",
       "2  [(Matthew C. Nisbet, PERSON), (Walter Lippmann...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### all together ########################\n",
    "import stanza\n",
    "import spacy\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "#stanza.download('en')\n",
    "nlp_stanza = stanza.Pipeline(lang=\"en\") \n",
    "nlp_spacy = spacy.load('en_core_web_sm')\n",
    "\n",
    "path = './Biographies/*'\n",
    "files = sum([glob.glob(dir+'/*.txt') for dir in glob.glob(path)],[]) # a commenter pour tout faire\n",
    "files = files[:3]\n",
    "\n",
    "data = []\n",
    "for file in files:\n",
    "    filename = file.split('/')[-1]\n",
    "    print(file)\n",
    "    with open(file, 'r') as f:\n",
    "        content = f.read()\n",
    "\n",
    "        doc_stanza = nlp_stanza(content)\n",
    "        ents_stanza = [(ent.text, ent.type) for ent in doc_stanza.ents]\n",
    "    \n",
    "        doc_spacy = nlp_spacy(content)\n",
    "        ents_spacy = [(ent.text, ent.label_) for ent in doc_spacy.ents]\n",
    "    \n",
    "        data.append({'file_name': filename,\n",
    "                    'ents_stanza': ents_stanza,\n",
    "                    'ents_spacy': ents_spacy})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674547a9-623b-414f-a9fd-074c277caf16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
