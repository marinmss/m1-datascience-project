{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d67785c-6003-402c-8c57-9e3a5420d0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    filename  \\\n",
      "0        AquileoJ.Echeverría_Journalists.txt   \n",
      "1  ThomasHenryShadwellClerke_Journalists.txt   \n",
      "2                 Journalist_Journalists.txt   \n",
      "3             KayodeAkintemi_Journalists.txt   \n",
      "4                  31Minutos_Journalists.txt   \n",
      "\n",
      "                                                ents  \n",
      "0  ((Aquileo, J., Echeverría), (May, 22, ,, 1866)...  \n",
      "1  ((Thomas, Henry, Shadwell, Clerke), (KH), (Iri...  \n",
      "2  ((Matthew, C., Nisbet), (Walter, Lippmann), (F...  \n",
      "3  ((Kayode, Akintemi), (June, 26, ,, 1965), (Nig...  \n",
      "4  ((31), (English), (31, minutes), (Chilean), (A...  \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "path = './Biographies/*'\n",
    "files = sum([glob.glob(dir+'/*.txt') for dir in glob.glob(path)],[])\n",
    "\n",
    "filenames = [file.split('/')[3] for file in files]\n",
    "\n",
    "ents=[]\n",
    "for file in files:\n",
    "    with open(file) as infile:\n",
    "        file_content =  infile.read()\n",
    "        doc = nlp(file_content)\n",
    "        ents.append(doc.ents)\n",
    "\n",
    "data = list(zip(filenames,ents))\n",
    "df = pd.DataFrame(data, columns = ['filename','ents'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e63d636d-17ef-4e85-ba1e-c7cfedd456ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marina/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 13.8MB/s]\n",
      "2024-06-11 15:22:51 INFO: Downloaded file to /home/marina/stanza_resources/resources.json\n",
      "2024-06-11 15:22:51 INFO: Downloading default packages for language: en (English) ...\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.8.0/models/default.zip: 100%|█| 527M/527M [00:17<0\n",
      "2024-06-11 15:23:11 INFO: Downloaded file to /home/marina/stanza_resources/en/default.zip\n",
      "2024-06-11 15:23:16 INFO: Finished downloading models and saved to /home/marina/stanza_resources\n",
      "2024-06-11 15:23:16 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 11.5MB/s]\n",
      "2024-06-11 15:23:17 INFO: Downloaded file to /home/marina/stanza_resources/resources.json\n",
      "2024-06-11 15:23:18 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-06-11 15:23:18 INFO: Using device: cpu\n",
      "2024-06-11 15:23:18 INFO: Loading: tokenize\n",
      "2024-06-11 15:23:19 INFO: Loading: mwt\n",
      "2024-06-11 15:23:19 INFO: Loading: pos\n",
      "2024-06-11 15:23:19 INFO: Loading: lemma\n",
      "2024-06-11 15:23:19 INFO: Loading: constituency\n",
      "2024-06-11 15:23:20 INFO: Loading: depparse\n",
      "2024-06-11 15:23:20 INFO: Loading: sentiment\n",
      "2024-06-11 15:23:20 INFO: Loading: ner\n",
      "2024-06-11 15:23:21 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline(lang=\"en\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdecdd6e-8dd0-4583-a001-8c06ba7f4413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
