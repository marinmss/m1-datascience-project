{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5e0f640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 5.05MB/s]\n",
      "2024-06-12 11:23:02 INFO: Downloaded file to /Users/abigail.berthe/stanza_resources/resources.json\n",
      "2024-06-12 11:23:02 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-06-12 11:23:04 INFO: File exists: /Users/abigail.berthe/stanza_resources/en/default.zip\n",
      "2024-06-12 11:23:17 INFO: Finished downloading models and saved to /Users/abigail.berthe/stanza_resources\n",
      "2024-06-12 11:23:17 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 23.9MB/s]\n",
      "2024-06-12 11:23:17 INFO: Downloaded file to /Users/abigail.berthe/stanza_resources/resources.json\n",
      "2024-06-12 11:23:17 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-06-12 11:23:18 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2024-06-12 11:23:18 INFO: Using device: cpu\n",
      "2024-06-12 11:23:18 INFO: Loading: tokenize\n",
      "2024-06-12 11:23:18 INFO: Loading: mwt\n",
      "2024-06-12 11:23:18 INFO: Loading: ner\n",
      "2024-06-12 11:23:19 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Biographies/Biographies_Journalists/SérgioPereiraCouto_Journalists.txt\n",
      "./Biographies/Biographies_Journalists/Gamalal-Ghitani_Journalists.txt\n",
      "./Biographies/Biographies_Journalists/J.R.RalphCasimir_Journalists.txt\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'loc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 33\u001b[0m\n\u001b[1;32m     28\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m'\u001b[39m: filename,\n\u001b[1;32m     29\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ments_stanza\u001b[39m\u001b[38;5;124m'\u001b[39m: ents_stanza,\n\u001b[1;32m     30\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ments_spacy\u001b[39m\u001b[38;5;124m'\u001b[39m: ents_spacy})\n\u001b[1;32m     32\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[0;32m---> 33\u001b[0m df\u001b[38;5;241m.\u001b[39mloc(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m[pd[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSérgioPereiraCouto_Journalists.txt\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     34\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'loc'"
     ]
    }
   ],
   "source": [
    "### all together ########################\n",
    "import stanza.pipeline\n",
    "import spacy\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "stanza.download('en')\n",
    "nlp_stanza = stanza.Pipeline('en', processors = 'tokenize,ner')#stanza.Pipeline(lang=\"en\") \n",
    "nlp_spacy = spacy.load('en_core_web_sm')\n",
    "\n",
    "path = './Biographies/*'\n",
    "files = sum([glob.glob(dir+'/*.txt') for dir in glob.glob(path)],[]) # a commenter pour tout faire\n",
    "files = files[:3]\n",
    "\n",
    "data = []\n",
    "for file in files:\n",
    "    filename = file.split('/')[-1]\n",
    "    print(file)\n",
    "    with open(file, 'r') as f:\n",
    "        content = f.read()\n",
    "\n",
    "        doc_stanza = nlp_stanza(content)\n",
    "        ents_stanza = [(ent.text, ent.type) for ent in doc_stanza.ents]\n",
    "    \n",
    "        doc_spacy = nlp_spacy(content)\n",
    "        ents_spacy = [(ent.text, ent.label_) for ent in doc_spacy.ents]\n",
    "    \n",
    "        data.append({'file_name': filename,\n",
    "                    'ents_stanza': ents_stanza,\n",
    "                    'ents_spacy': ents_spacy})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e6cfa1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\"\"\"\n",
    "• Write a function that checks one document (i.e. a biography) for\n",
    "the following:\n",
    "(a) the number of spans (i.e. token(s)) where both packages agree and predict is an NE (i.e. complete overlap in span predicted).\n",
    "(b) the number of spans where there is a partial agreement between both packages (i.e. partial overlap in spans predicted).\n",
    "(c) for each package, the number of spans that a package predicted as an NE, but the other package did not predict as an NE.\n",
    "(d) for the spans with full and partial agreement, was there an agreement in the NE type (e.g. Person, Location, Organisation etc)\n",
    "(e) Use visualisation to compare the above statistics, per category per package (i.e. Spacy vs Stanza)\n",
    "\"\"\"\n",
    "\n",
    "def complete_overlap(df_docrow):\n",
    "    \"\"\"\n",
    "    (a) the number of spans (i.e. token(s)) where both packages agree and predict is an NE (i.e. complete overlap in span predicted)\n",
    "    \"\"\"\n",
    "    ents_stanza = [ df_docrow['ents_stanza'][0][x][0] for x in range(len(df_docrow['ents_stanza'][0]))]\n",
    "    ents_spacy = [ df_docrow['ents_spacy'][0][x][0] for x in range(len(df_docrow['ents_spacy'][0]))]\n",
    "    ents_both = [ent for ent in ents_stanza if ent in ents_spacy]\n",
    "    #print(f\"{ents_stanza}\\n\\n{ents_spacy}\\n\\n{ents_both}\")\n",
    "    #print(ents_both)\n",
    "    return ents_both\n",
    "\n",
    "def partial_overlap(df_docrow):\n",
    "    \"\"\"\n",
    "    (b) the number of spans where there is a partial agreement between both packages (i.e. partial overlap in spans predicted).   \n",
    "    PARTIAL strictly (perfect matches are removed)\n",
    "    \"\"\"\n",
    "    ents_stanza = [ df_docrow['ents_stanza'][0][x][0] for x in range(len(df_docrow['ents_stanza'][0]))]\n",
    "    ents_spacy = [ df_docrow['ents_spacy'][0][x][0] for x in range(len(df_docrow['ents_spacy'][0]))]\n",
    "    complete = complete_overlap(df_docrow)\n",
    "    ents_both_partial = [ent for ent in ents_stanza if ent in \" \".join(ents_spacy) and ent not in complete]\n",
    "    ents_both_partial.extend([ent for ent in ents_spacy if ent in \" \".join(ents_stanza) and ent not in ents_both_partial and ent not in complete])\n",
    "    #print(ents_both_partial)\n",
    "    return ents_both_partial\n",
    "\n",
    "def one_but_not_the_other(df_docrow):\n",
    "    \"\"\"\n",
    "    (c) for each package, the number of spans that a package predicted as an NE, but the other package did not predict as an NE.\n",
    "    \"\"\"\n",
    "    ents_stanza = [ df_docrow['ents_stanza'][0][x][0] for x in range(len(df_docrow['ents_stanza'][0]))]\n",
    "    ents_spacy = [ df_docrow['ents_spacy'][0][x][0] for x in range(len(df_docrow['ents_spacy'][0]))]\n",
    "    elts_both_lists = complete_overlap(df_docrow) #elements that are in both lists (complete element)\n",
    "    elts_both_lists_partial = partial_overlap(df_docrow)\n",
    "    #print(elts_both_lists)\n",
    "    merged_lists = ents_stanza\n",
    "    merged_lists.extend(ents_spacy)\n",
    "    #print(list(set(merged_lists)))\n",
    "    only_in_one_complete= [ent for ent in merged_lists if ent not in elts_both_lists] # yields the elements in `merged_lists` that are NOT in `elts_both_lists`\n",
    "    only_in_one_partial= [ent for ent in only_in_one_complete if ent not in elts_both_lists_partial]\n",
    "    return only_in_one_complete, only_in_one_partial\n",
    "\n",
    "def find_tuple_with_first_value(tuples_list, specific_value):\n",
    "    \"\"\"Returns the tuple with the specific_value as the first element if found, else None.\"\"\"\n",
    "    for tup in tuples_list:\n",
    "        if tup[0] == specific_value:\n",
    "            return tup\n",
    "    return None\n",
    "\n",
    "def find_tuple_containing_value(tuples_list, specific_value):\n",
    "    \"\"\"Returns the tuple with the specific_value as the first element if found, else None.\"\"\"\n",
    "    for tup in tuples_list:\n",
    "        if specific_value in tup[0] :\n",
    "            return tup\n",
    "    return None\n",
    "\n",
    "def agreement_ne_type_complete(df_docrow):\n",
    "    \"\"\"\n",
    "    (d) for the spans with full agreement, was there an agreement in the NE type (e.g. Person, Location, Organisation etc)\n",
    "    \"\"\"\n",
    "    spans_agreement = complete_overlap(df_docrow)\n",
    "\n",
    "    row_span_spacy = [ df_docrow['ents_stanza'][0][x] for x in range(len(df_docrow['ents_stanza'][0]))]\n",
    "    row_span_stanza = [ df_docrow['ents_spacy'][0][x] for x in range(len(df_docrow['ents_spacy'][0]))]\n",
    "    agree = 0\n",
    "    disagree = 0\n",
    "    for span in spans_agreement:\n",
    "        if(find_tuple_with_first_value(row_span_spacy, span)[1] == find_tuple_with_first_value(row_span_stanza, span)[1]):\n",
    "            agree +=1\n",
    "        else:\n",
    "            disagree +=1\n",
    "    \n",
    "        \n",
    "    return agree, disagree\n",
    "\n",
    "def agreement_ne_type_partial(df_docrow):\n",
    "    \"\"\"\n",
    "    (d) for the spans with partial agreement, was there an agreement in the NE type (e.g. Person, Location, Organisation etc)\n",
    "    \"\"\"\n",
    "    spans_agreement = partial_overlap(df_docrow)\n",
    "\n",
    "    row_span_spacy = [ df_docrow['ents_stanza'][0][x] for x in range(len(df_docrow['ents_stanza'][0]))]\n",
    "    row_span_stanza = [ df_docrow['ents_spacy'][0][x] for x in range(len(df_docrow['ents_spacy'][0]))]\n",
    "    agree = 0\n",
    "    disagree = 0\n",
    "    for span in spans_agreement:\n",
    "            if(find_tuple_with_first_value(row_span_spacy, span) != None and find_tuple_with_first_value(row_span_spacy, span)[1] == find_tuple_containing_value(row_span_stanza, span)[1]):\n",
    "                agree +=1\n",
    "            elif(find_tuple_with_first_value(row_span_stanza, span) != None and find_tuple_with_first_value(row_span_stanza, span)[1] == find_tuple_containing_value(row_span_spacy, span)[1]):\n",
    "                agree +=1\n",
    "            else:\n",
    "                disagree +=1\n",
    "        \n",
    "    return agree, disagree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b116ca29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of where both packages agree and predict is a NE : 5\n",
      "\n",
      "\n",
      "\n",
      "Number of spans where there is a partial agreement between both packages: 42\n",
      "\n",
      "\n",
      "\n",
      "Number of spans that a package predicted as an NE, but the other package did not predict as a NE (only exact matches): 59\n",
      "Number of spans that a package predicted as an NE, but the other package did not predict as a NE (with partial matches): 17\n",
      "\n",
      "\n",
      "Total agreement for NE: \n",
      "Number of span agreeing on the NE type : 3\n",
      "Number of span disagreeing on the NE type : 2\n",
      "\n",
      "Partial agreement for NE: \n",
      "Number of span agreeing on the NE type : 5\n",
      "Number of span disagreeing on the NE type : 37\n"
     ]
    }
   ],
   "source": [
    "def comparison_stanza_spacy(pd_df, doc):\n",
    "    row_doc = pd_df.loc[pd_df['file_name'] == doc]\n",
    "    print(f\"Number of where both packages agree and predict is a NE : {len(complete_overlap(row_doc))}\")\n",
    "    print('\\n\\n')\n",
    "    print(f\"Number of spans where there is a partial agreement between both packages: {len(partial_overlap(row_doc))}\")\n",
    "    print('\\n\\n')\n",
    "    complete_one, partial_one = one_but_not_the_other(row_doc)\n",
    "    print(f\"Number of spans that a package predicted as an NE, but the other package did not predict as a NE (only exact matches): {len(complete_one)}\")\n",
    "    print(f\"Number of spans that a package predicted as an NE, but the other package did not predict as a NE (with partial matches): {len(partial_one)}\")\n",
    "    total_agree, total_disagree = agreement_ne_type_complete(pd_df)\n",
    "    partial_agree, partial_disagree = agreement_ne_type_partial(pd_df)\n",
    "    print(\"\\n\\nTotal agreement for NE: \")\n",
    "    print(f\"Number of span agreeing on the NE type : {total_agree}\")\n",
    "    print(f\"Number of span disagreeing on the NE type : {total_disagree}\")\n",
    "    print(\"\\nPartial agreement for NE: \")\n",
    "    print(f\"Number of span agreeing on the NE type : {partial_agree}\")\n",
    "    print(f\"Number of span disagreeing on the NE type : {partial_disagree}\")\n",
    "                        \n",
    "    \n",
    "    \n",
    "comparison_stanza_spacy(df, 'SérgioPereiraCouto_Journalists.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c0528112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5fa3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
