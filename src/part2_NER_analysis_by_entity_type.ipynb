{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9543c8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 5.05MB/s]\n",
      "2024-06-12 11:23:02 INFO: Downloaded file to /Users/abigail.berthe/stanza_resources/resources.json\n",
      "2024-06-12 11:23:02 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-06-12 11:23:04 INFO: File exists: /Users/abigail.berthe/stanza_resources/en/default.zip\n",
      "2024-06-12 11:23:17 INFO: Finished downloading models and saved to /Users/abigail.berthe/stanza_resources\n",
      "2024-06-12 11:23:17 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 23.9MB/s]\n",
      "2024-06-12 11:23:17 INFO: Downloaded file to /Users/abigail.berthe/stanza_resources/resources.json\n",
      "2024-06-12 11:23:17 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-06-12 11:23:18 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2024-06-12 11:23:18 INFO: Using device: cpu\n",
      "2024-06-12 11:23:18 INFO: Loading: tokenize\n",
      "2024-06-12 11:23:18 INFO: Loading: mwt\n",
      "2024-06-12 11:23:18 INFO: Loading: ner\n",
      "2024-06-12 11:23:19 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Biographies/Biographies_Journalists/SérgioPereiraCouto_Journalists.txt\n",
      "./Biographies/Biographies_Journalists/Gamalal-Ghitani_Journalists.txt\n",
      "./Biographies/Biographies_Journalists/J.R.RalphCasimir_Journalists.txt\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'loc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 33\u001b[0m\n\u001b[1;32m     28\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m'\u001b[39m: filename,\n\u001b[1;32m     29\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ments_stanza\u001b[39m\u001b[38;5;124m'\u001b[39m: ents_stanza,\n\u001b[1;32m     30\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ments_spacy\u001b[39m\u001b[38;5;124m'\u001b[39m: ents_spacy})\n\u001b[1;32m     32\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[0;32m---> 33\u001b[0m df\u001b[38;5;241m.\u001b[39mloc(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m[pd[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSérgioPereiraCouto_Journalists.txt\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     34\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'loc'"
     ]
    }
   ],
   "source": [
    "### all together ########################\n",
    "import stanza.pipeline\n",
    "import spacy\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "stanza.download('en')\n",
    "nlp_stanza = stanza.Pipeline('en', processors = 'tokenize,ner')#stanza.Pipeline(lang=\"en\") \n",
    "nlp_spacy = spacy.load('en_core_web_sm')\n",
    "\n",
    "path = './Biographies/*'\n",
    "files = sum([glob.glob(dir+'/*.txt') for dir in glob.glob(path)],[]) # a commenter pour tout faire\n",
    "files = files[:3]\n",
    "\n",
    "data = []\n",
    "for file in files:\n",
    "    filename = file.split('/')[-1]\n",
    "    print(file)\n",
    "    with open(file, 'r') as f:\n",
    "        content = f.read()\n",
    "\n",
    "        doc_stanza = nlp_stanza(content)\n",
    "        ents_stanza = [(ent.text, ent.type) for ent in doc_stanza.ents]\n",
    "    \n",
    "        doc_spacy = nlp_spacy(content)\n",
    "        ents_spacy = [(ent.text, ent.label_) for ent in doc_spacy.ents]\n",
    "    \n",
    "        data.append({'file_name': filename,\n",
    "                    'ents_stanza': ents_stanza,\n",
    "                    'ents_spacy': ents_spacy})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc4d7e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sérgio Pereira Couto', '6 March 1967', 'Ciência Criminal', 'Dossiê Hitler', 'José Antonio Domingos']\n",
      "\n",
      "\n",
      "\n",
      "['Investigação Criminal\\nOs Heróis de Esparta\\nRenascimento', 'Portuguese', 'Brazilian', 'Brasil', 'Geek', 'Galileu', 'Criminais', '2', 'Sociedades Secretas: A', 'Verdade Sobre', 'Código da Vinci\\nDecifrando a Fortaleza Digital', 'Secretas', 'Maçonaria', 'Secretas', 'Secreta', 'Piratas', 'Evangelho de Judas', 'A História Secreta de Roma\\nSeitas Secretas', 'Maçonaria', 'Para Não-Iniciados', 'Os Segredos', 'Segredos', 'Dez Sociedades', 'da História', 'Secretas', 'Illuminati', 'Hitler', 'Segredos', '1', '2', 'Almanaque', 'Decifrando', 'Símbolo Perdido', 'Maçonaria', 'John Lennon', '2012', 'Arquivos Secretos', 'Vaticano', 'Homem Que Previa', 'Futuro\\nAlmanaque das Sociedades', 'Secretas', 'WikiLeaks']\n",
      "\n",
      "\n",
      "\n",
      "[('Sérgio Pereira Couto', 'ORG'), ('6 March 1967', 'DATE'), ('Portuguese', 'NORP'), ('Brazilian', 'NORP'), ('Ciência Criminal', 'PERSON'), ('Brasil', 'PERSON'), ('Geek', 'PERSON'), ('Galileu', 'GPE'), ('Investigação Criminal\\nOs Heróis de Esparta\\nRenascimento\\nSociedades Secretas', 'FAC'), ('Lenda', 'NORP'), ('Beatlemaníaco', 'GPE'), ('Jogos', 'GPE'), ('Criminais', 'GPE'), ('2', 'CARDINAL'), ('Sociedades Secretas: A', 'WORK_OF_ART'), ('Verdade Sobre', 'PERSON'), ('Código da Vinci\\nDecifrando a Fortaleza Digital', 'PERSON'), ('Secretas', 'PERSON'), ('Maçonaria', 'GPE'), ('Secretas', 'PERSON'), ('Secreta', 'PERSON'), ('Piratas', 'GPE'), ('Evangelho de Judas', 'PERSON'), ('A História Secreta de Roma\\nSeitas Secretas', 'WORK_OF_ART'), ('Maçonaria', 'GPE'), ('Para Não-Iniciados', 'PERSON'), ('Dossiê Hitler', 'PERSON'), ('Egito', 'PERSON'), ('Os Segredos', 'NORP'), ('Códigos e Cifras', 'ORG'), ('Segredos', 'NORP'), ('Dez Sociedades', 'PERSON'), ('da História', 'PERSON'), ('Secretas', 'PERSON'), ('Illuminati', 'NORP'), ('Hitler', 'PERSON'), ('Segredos', 'NORP'), ('1', 'CARDINAL'), ('2', 'CARDINAL'), ('Almanaque', 'NORP'), ('Decifrando', 'PERSON'), ('Símbolo Perdido', 'PERSON'), ('Maçonaria', 'GPE'), ('John Lennon', 'PERSON'), ('2012', 'DATE'), ('Arquivos Secretos', 'PERSON'), ('Vaticano', 'PERSON'), ('Homem Que Previa', 'PERSON'), ('Futuro\\nAlmanaque das Sociedades', 'PERSON'), ('Secretas', 'ORG'), ('WikiLeaks', 'PRODUCT'), ('José Antonio Domingos', 'PERSON')]\n",
      "\n",
      "\n",
      "\n",
      "[('Sérgio Pereira Couto', 'PERSON'), ('6 March 1967', 'DATE'), ('Portuguese-Brazilian', 'NORP'), ('Ciência Criminal', 'ORG'), ('Discovery Magazine', 'ORG'), ('PC Brasil', 'ORG'), ('Geek!', 'ORG'), ('Galileu, Planeta', 'ORG'), ('Investigação Criminal\\nOs Heróis de Esparta\\nRenascimento', 'WORK_OF_ART'), ('2\\nAnno Domini 2', 'DATE'), ('Batalha dos Deuses', 'WORK_OF_ART'), ('Sociedades Secretas: A Verdade Sobre o Código da Vinci\\nDecifrando a Fortaleza Digital\\nSociedades Secretas: Maçonaria\\nSociedades Secretas: Templários\\nDicionário Secreto da Maçonaria\\nA História Secreta dos Piratas\\nEvangelho de Judas e Outros Mistérios\\nA História Secreta de Roma\\nSeitas Secretas\\nMaçonaria Para Não-Iniciados', 'WORK_OF_ART'), ('Dossiê Hitler', 'PERSON'), ('Extraordinária História da China\\nCódigos', 'WORK_OF_ART'), ('Dez Sociedades Mais Influentes da História\\nSociedades Secretas: Illuminati\\nOs Segredos das Investigações Criminais\\nHitler e os Segredos do Nazismo', 'WORK_OF_ART'), ('Símbolo Perdido\\nManual de Investigação Forense\\nDesvendando a Maçonaria\\nDossiê John Lennon\\n2012 X Nostradamus\\nArquivos Secretos do Vaticano\\nO Homem Que Previa o Futuro\\nAlmanaque das Sociedades Secretas\\nWikiLeaks: Segredos, Informações e Poder', 'WORK_OF_ART'), ('José Antonio Domingos', 'PERSON')]\n",
      "\n",
      "\n",
      "\n",
      "(['Portuguese-Brazilian', 'Discovery Magazine', 'PC Brasil', 'Geek!', 'Galileu, Planeta', 'Investigação Criminal\\nOs Heróis de Esparta\\nRenascimento', '2\\nAnno Domini 2', 'Batalha dos Deuses', 'Sociedades Secretas: A Verdade Sobre o Código da Vinci\\nDecifrando a Fortaleza Digital\\nSociedades Secretas: Maçonaria\\nSociedades Secretas: Templários\\nDicionário Secreto da Maçonaria\\nA História Secreta dos Piratas\\nEvangelho de Judas e Outros Mistérios\\nA História Secreta de Roma\\nSeitas Secretas\\nMaçonaria Para Não-Iniciados', 'Extraordinária História da China\\nCódigos', 'Dez Sociedades Mais Influentes da História\\nSociedades Secretas: Illuminati\\nOs Segredos das Investigações Criminais\\nHitler e os Segredos do Nazismo', 'Símbolo Perdido\\nManual de Investigação Forense\\nDesvendando a Maçonaria\\nDossiê John Lennon\\n2012 X Nostradamus\\nArquivos Secretos do Vaticano\\nO Homem Que Previa o Futuro\\nAlmanaque das Sociedades Secretas\\nWikiLeaks: Segredos, Informações e Poder', 'Portuguese', 'Brazilian', 'Brasil', 'Geek', 'Galileu', 'Investigação Criminal\\nOs Heróis de Esparta\\nRenascimento\\nSociedades Secretas', 'Lenda', 'Beatlemaníaco', 'Jogos', 'Criminais', '2', 'Sociedades Secretas: A', 'Verdade Sobre', 'Código da Vinci\\nDecifrando a Fortaleza Digital', 'Secretas', 'Maçonaria', 'Secretas', 'Secreta', 'Piratas', 'Evangelho de Judas', 'A História Secreta de Roma\\nSeitas Secretas', 'Maçonaria', 'Para Não-Iniciados', 'Egito', 'Os Segredos', 'Códigos e Cifras', 'Segredos', 'Dez Sociedades', 'da História', 'Secretas', 'Illuminati', 'Hitler', 'Segredos', '1', '2', 'Almanaque', 'Decifrando', 'Símbolo Perdido', 'Maçonaria', 'John Lennon', '2012', 'Arquivos Secretos', 'Vaticano', 'Homem Que Previa', 'Futuro\\nAlmanaque das Sociedades', 'Secretas', 'WikiLeaks'], ['Portuguese-Brazilian', 'Discovery Magazine', 'PC Brasil', 'Geek!', 'Galileu, Planeta', '2\\nAnno Domini 2', 'Batalha dos Deuses', 'Sociedades Secretas: A Verdade Sobre o Código da Vinci\\nDecifrando a Fortaleza Digital\\nSociedades Secretas: Maçonaria\\nSociedades Secretas: Templários\\nDicionário Secreto da Maçonaria\\nA História Secreta dos Piratas\\nEvangelho de Judas e Outros Mistérios\\nA História Secreta de Roma\\nSeitas Secretas\\nMaçonaria Para Não-Iniciados', 'Extraordinária História da China\\nCódigos', 'Dez Sociedades Mais Influentes da História\\nSociedades Secretas: Illuminati\\nOs Segredos das Investigações Criminais\\nHitler e os Segredos do Nazismo', 'Símbolo Perdido\\nManual de Investigação Forense\\nDesvendando a Maçonaria\\nDossiê John Lennon\\n2012 X Nostradamus\\nArquivos Secretos do Vaticano\\nO Homem Que Previa o Futuro\\nAlmanaque das Sociedades Secretas\\nWikiLeaks: Segredos, Informações e Poder', 'Investigação Criminal\\nOs Heróis de Esparta\\nRenascimento\\nSociedades Secretas', 'Lenda', 'Beatlemaníaco', 'Jogos', 'Egito', 'Códigos e Cifras'])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\"\"\"\n",
    "• Write a function that checks one document (i.e. a biography) for\n",
    "the following:\n",
    "(a) the number of spans (i.e. token(s)) where both packages agree and predict is an NE (i.e. complete overlap in span predicted).\n",
    "(b) the number of spans where there is a partial agreement between both packages (i.e. partial overlap in spans predicted).\n",
    "(c) for each package, the number of spans that a package predicted as an NE, but the other package did not predict as an NE.\n",
    "(d) for the spans with full and partial agreement, was there an agreement in the NE type (e.g. Person, Location, Organisation etc)\n",
    "(e) Use visualisation to compare the above statistics, per category per package (i.e. Spacy vs Stanza)\n",
    "\"\"\"\n",
    "\n",
    "def complete_overlap(df_docrow):\n",
    "    \"\"\"\n",
    "    (a) the number of spans (i.e. token(s)) where both packages agree and predict is an NE (i.e. complete overlap in span predicted)\n",
    "    \"\"\"\n",
    "    ents_stanza = [ df_docrow['ents_stanza'][0][x][0] for x in range(len(df_docrow['ents_stanza'][0]))]\n",
    "    ents_spacy = [ df_docrow['ents_spacy'][0][x][0] for x in range(len(df_docrow['ents_spacy'][0]))]\n",
    "    ents_both = [ent for ent in ents_stanza if ent in ents_spacy]\n",
    "    #print(f\"{ents_stanza}\\n\\n{ents_spacy}\\n\\n{ents_both}\")\n",
    "    #print(ents_both)\n",
    "    return ents_both\n",
    "\n",
    "def partial_overlap(df_docrow):\n",
    "    \"\"\"\n",
    "    (b) the number of spans where there is a partial agreement between both packages (i.e. partial overlap in spans predicted).   \n",
    "    PARTIAL strictly (perfect matches are removed)\n",
    "    \"\"\"\n",
    "    ents_stanza = [ df_docrow['ents_stanza'][0][x][0] for x in range(len(df_docrow['ents_stanza'][0]))]\n",
    "    ents_spacy = [ df_docrow['ents_spacy'][0][x][0] for x in range(len(df_docrow['ents_spacy'][0]))]\n",
    "    complete = complete_overlap(df_docrow)\n",
    "    ents_both_partial = [ent for ent in ents_stanza if ent in \" \".join(ents_spacy) and ent not in complete]\n",
    "    ents_both_partial.extend([ent for ent in ents_spacy if ent in \" \".join(ents_stanza) and ent not in ents_both_partial and ent not in complete])\n",
    "    #print(ents_both_partial)\n",
    "    return ents_both_partial\n",
    "\n",
    "def one_but_not_the_other(df_docrow):\n",
    "    \"\"\"\n",
    "    (c) for each package, the number of spans that a package predicted as an NE, but the other package did not predict as an NE.\n",
    "    \"\"\"\n",
    "    ents_stanza = [ df_docrow['ents_stanza'][0][x][0] for x in range(len(df_docrow['ents_stanza'][0]))]\n",
    "    ents_spacy = [ df_docrow['ents_spacy'][0][x][0] for x in range(len(df_docrow['ents_spacy'][0]))]\n",
    "    elts_both_lists = complete_overlap(df_docrow) #elements that are in both lists (complete element)\n",
    "    elts_both_lists_partial = partial_overlap(df_docrow)\n",
    "    #print(elts_both_lists)\n",
    "    merged_lists = ents_stanza\n",
    "    merged_lists.extend(ents_spacy)\n",
    "    #print(list(set(merged_lists)))\n",
    "    only_in_one_complete= [ent for ent in merged_lists if ent not in elts_both_lists] # yields the elements in `merged_lists` that are NOT in `elts_both_lists`\n",
    "    only_in_one_partial= [ent for ent in only_in_one_complete if ent not in elts_both_lists_partial]\n",
    "    return only_in_one_complete, only_in_one_partial\n",
    "\n",
    "def agreement_ne_type(df_docrow):\n",
    "    \"\"\"\n",
    "    (d) for the spans with full and partial agreement, was there an agreement in the NE type (e.g. Person, Location, Organisation etc)\n",
    "    \"\"\"\n",
    "    return null\n",
    "\n",
    "def comparison_stanza_spacy(pd_df, doc):\n",
    "    row_doc = pd_df.loc[pd_df['file_name'] == doc]\n",
    "    print(complete_overlap(row_doc))\n",
    "    print('\\n\\n')\n",
    "    print(partial_overlap(row_doc))\n",
    "    print('\\n\\n')\n",
    "    print(pd_df.loc[pd_df['file_name'] == doc]['ents_spacy'][0])\n",
    "    print('\\n\\n')\n",
    "    print(pd_df.loc[pd_df['file_name'] == doc]['ents_stanza'][0])\n",
    "    print('\\n\\n')\n",
    "    print(one_but_not_the_other(row_doc))\n",
    "                        \n",
    "                        \n",
    "    #print(type(pd_df.loc[pd_df['file_name'] == doc]['ents_stanza'][0][0]))\n",
    "    #print(pd_df.loc[pd_df['file_name'] == doc]['ents_spacy'][0])\n",
    "    \n",
    "    \n",
    "comparison_stanza_spacy(df, 'SérgioPereiraCouto_Journalists.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f02eba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
